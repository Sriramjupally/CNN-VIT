{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom tqdm import tqdm","metadata":{"id":"Ct4gyUeeMyog","execution":{"iopub.status.busy":"2024-03-29T13:37:06.880299Z","iopub.execute_input":"2024-03-29T13:37:06.881229Z","iopub.status.idle":"2024-03-29T13:37:14.260569Z","shell.execute_reply.started":"2024-03-29T13:37:06.881194Z","shell.execute_reply":"2024-03-29T13:37:14.259514Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nnum_classes = 10\nnum_epochs = 10\nbatch_size = 16\nlearning_rate = 0.01\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root = './data', download = True, train = True, transform = transform)\ntest_dataset = torchvision.datasets.CIFAR10(root = './data', train = False, transform = transform)","metadata":{"id":"xb2gRLAMLRLS","outputId":"9ea142be-1c2e-4148-f5ae-c9cfb5c045b3","execution":{"iopub.status.busy":"2024-03-29T13:37:24.040894Z","iopub.execute_input":"2024-03-29T13:37:24.041690Z","iopub.status.idle":"2024-03-29T13:37:29.534651Z","shell.execute_reply.started":"2024-03-29T13:37:24.041659Z","shell.execute_reply":"2024-03-29T13:37:29.533560Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 80764732.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)","metadata":{"id":"i4XlTTARelYV","execution":{"iopub.status.busy":"2024-03-29T13:37:30.204678Z","iopub.execute_input":"2024-03-29T13:37:30.205370Z","iopub.status.idle":"2024-03-29T13:37:30.210960Z","shell.execute_reply.started":"2024-03-29T13:37:30.205336Z","shell.execute_reply":"2024-03-29T13:37:30.209781Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class self_attention_cnn(nn.Module):\n\n    def __init__(self, old, new, gamma):\n        super(self_attention_cnn, self).__init__()\n        self.query = nn.Linear(old, new)\n        self.key = nn.Linear(old, new)\n        self.value = nn.Linear(old, new)\n        self.residue = nn.Linear(new, old)\n        self.gamma = gamma\n\n    def forward(self, x):\n\n        ret = torch.zeros(x.shape).to(device)\n\n        for i in range(x.shape[0]):\n            size = x[i].shape\n\n#             print(x[i].shape)\n\n            f = self.query((x[i].permute(1, 2, 0)).view(-1, *size[0:1]))\n            g = self.key((x[i].permute(1, 2, 0)).view(-1, *size[0:1]))\n            h = self.value((x[i].permute(1, 2, 0)).view(-1, *size[0:1]))\n            qkt = f@g.transpose(0,1)\n            sm = F.softmax(qkt, dim = 1)\n            y = sm@h\n            transformed_y = self.residue(y)\n            o = self.gamma*transformed_y + (x[i].permute(1, 2, 0)).view(-1, *size[0:1])\n            ret[i] = o.view(tuple(size[i] for i in [1,2,0])).permute(2, 0, 1)\n\n        return ret\n\nclass conv_sa(nn.Module):\n    def __init__(self):\n        super(conv_sa, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 32, 5)  \n        self.Relu = nn.ReLU()\n        self.SA1 = self_attention_cnn(32, 4, 1)\n        self.conv2 = nn.Conv2d(32, 64, 5)\n        self.SA2 = self_attention_cnn(64, 8, 1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(64, 128, 5)\n        self.SA3 = self_attention_cnn(128, 16, 1)\n        self.conv4 = nn.Conv2d(128, 256, 5)\n        self.SA4 = self_attention_cnn(256, 32, 1)\n        \n        self.gap = nn.AvgPool2d(4,4)\n        \n        self.l1 = nn.Linear(256, 120)\n        self.l2 = nn.Linear(120, 84)\n        self.l3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        out = self.Relu(self.conv1(x))\n        out = self.SA1(out)\n        out = self.Relu(self.conv2(out))\n        out = self.SA2(out)\n        out = self.pool(out)\n        out = self.Relu(self.conv3(out))\n        out = self.SA3(out)\n        out = self.Relu(self.conv4(out))\n        out = self.SA4(out)\n        out = self.gap(out)\n        out = out.view((batch_size, 256))\n        \n        out = self.Relu(self.l1(out))\n        out = self.Relu(self.l2(out))\n        out = self.l3(out)\n        return out\n    \n\nModel = conv_sa().to(device)","metadata":{"id":"ST3feK_ZeOoI","execution":{"iopub.status.busy":"2024-03-29T13:37:34.194486Z","iopub.execute_input":"2024-03-29T13:37:34.194824Z","iopub.status.idle":"2024-03-29T13:37:34.467072Z","shell.execute_reply.started":"2024-03-29T13:37:34.194800Z","shell.execute_reply":"2024-03-29T13:37:34.466238Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"Model","metadata":{"execution":{"iopub.status.busy":"2024-03-29T13:37:39.017566Z","iopub.execute_input":"2024-03-29T13:37:39.017944Z","iopub.status.idle":"2024-03-29T13:37:39.025325Z","shell.execute_reply.started":"2024-03-29T13:37:39.017902Z","shell.execute_reply":"2024-03-29T13:37:39.024326Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"conv_sa(\n  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n  (Relu): ReLU()\n  (SA1): self_attention_cnn(\n    (query): Linear(in_features=32, out_features=4, bias=True)\n    (key): Linear(in_features=32, out_features=4, bias=True)\n    (value): Linear(in_features=32, out_features=4, bias=True)\n    (residue): Linear(in_features=4, out_features=32, bias=True)\n  )\n  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (SA2): self_attention_cnn(\n    (query): Linear(in_features=64, out_features=8, bias=True)\n    (key): Linear(in_features=64, out_features=8, bias=True)\n    (value): Linear(in_features=64, out_features=8, bias=True)\n    (residue): Linear(in_features=8, out_features=64, bias=True)\n  )\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n  (SA3): self_attention_cnn(\n    (query): Linear(in_features=128, out_features=16, bias=True)\n    (key): Linear(in_features=128, out_features=16, bias=True)\n    (value): Linear(in_features=128, out_features=16, bias=True)\n    (residue): Linear(in_features=16, out_features=128, bias=True)\n  )\n  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n  (SA4): self_attention_cnn(\n    (query): Linear(in_features=256, out_features=32, bias=True)\n    (key): Linear(in_features=256, out_features=32, bias=True)\n    (value): Linear(in_features=256, out_features=32, bias=True)\n    (residue): Linear(in_features=32, out_features=256, bias=True)\n  )\n  (gap): AvgPool2d(kernel_size=4, stride=4, padding=0)\n  (l1): Linear(in_features=256, out_features=120, bias=True)\n  (l2): Linear(in_features=120, out_features=84, bias=True)\n  (l3): Linear(in_features=84, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(Model.parameters(), lr =learning_rate)\n\n\ntotal_steps = len(train_loader)\n\nfor epoch in range(num_epochs):\n    for i , (images, label) in enumerate(train_loader):\n\n        images = images.to(device)\n        label = label.to(device)\n\n        outputs = Model(images)\n        loss = criterion(outputs, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n#         print(i)\n\n        if(i+1) % 1000 == 0:\n            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{total_steps}, loss = {loss.item():5f}')","metadata":{"id":"T36ski0Vlf2y","execution":{"iopub.status.busy":"2024-03-29T13:37:47.350759Z","iopub.execute_input":"2024-03-29T13:37:47.351151Z","iopub.status.idle":"2024-03-29T14:25:01.166519Z","shell.execute_reply.started":"2024-03-29T13:37:47.351121Z","shell.execute_reply":"2024-03-29T14:25:01.165619Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"epoch 1 / 10, step 1000/3125, loss = 2.291237\nepoch 1 / 10, step 2000/3125, loss = 2.012730\nepoch 1 / 10, step 3000/3125, loss = 1.949450\nepoch 2 / 10, step 1000/3125, loss = 1.730939\nepoch 2 / 10, step 2000/3125, loss = 1.922977\nepoch 2 / 10, step 3000/3125, loss = 1.479888\nepoch 3 / 10, step 1000/3125, loss = 1.648847\nepoch 3 / 10, step 2000/3125, loss = 1.814000\nepoch 3 / 10, step 3000/3125, loss = 1.129269\nepoch 4 / 10, step 1000/3125, loss = 1.220394\nepoch 4 / 10, step 2000/3125, loss = 1.048416\nepoch 4 / 10, step 3000/3125, loss = 1.024386\nepoch 5 / 10, step 1000/3125, loss = 1.205450\nepoch 5 / 10, step 2000/3125, loss = 0.730433\nepoch 5 / 10, step 3000/3125, loss = 1.156644\nepoch 6 / 10, step 1000/3125, loss = 1.245951\nepoch 6 / 10, step 2000/3125, loss = 0.982236\nepoch 6 / 10, step 3000/3125, loss = 1.050127\nepoch 7 / 10, step 1000/3125, loss = 0.656318\nepoch 7 / 10, step 2000/3125, loss = 0.557695\nepoch 7 / 10, step 3000/3125, loss = 0.679904\nepoch 8 / 10, step 1000/3125, loss = 1.025865\nepoch 8 / 10, step 2000/3125, loss = 1.010907\nepoch 8 / 10, step 3000/3125, loss = 0.501253\nepoch 9 / 10, step 1000/3125, loss = 0.373130\nepoch 9 / 10, step 2000/3125, loss = 0.350843\nepoch 9 / 10, step 3000/3125, loss = 0.723451\nepoch 10 / 10, step 1000/3125, loss = 0.735688\nepoch 10 / 10, step 2000/3125, loss = 0.307848\nepoch 10 / 10, step 3000/3125, loss = 0.799589\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n\n    n_c = 0\n    n_t = 0\n    \n    op = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = Model(images)\n#         op = outputs\n#         break\n\n        _, predictions = torch.max(outputs.data, 1)\n        n_t += labels.shape[0]\n        n_c += (predictions == labels).sum().item()\n\n    acc = 100*n_c/n_t\n    print(f'acc: {acc:4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-29T14:25:15.241105Z","iopub.execute_input":"2024-03-29T14:25:15.242010Z","iopub.status.idle":"2024-03-29T14:26:49.195861Z","shell.execute_reply.started":"2024-03-29T14:25:15.241975Z","shell.execute_reply":"2024-03-29T14:26:49.194903Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"acc: 81.682000\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n\n    n_c = 0\n    n_t = 0\n    \n    op = 0\n\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = Model(images)\n#         op = outputs\n#         break\n\n        _, predictions = torch.max(outputs.data, 1)\n        n_t += labels.shape[0]\n        n_c += (predictions == labels).sum().item()\n\n    acc = 100*n_c/n_t\n    print(f'acc: {acc:4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-29T14:27:06.470794Z","iopub.execute_input":"2024-03-29T14:27:06.471161Z","iopub.status.idle":"2024-03-29T14:27:25.265946Z","shell.execute_reply.started":"2024-03-29T14:27:06.471133Z","shell.execute_reply":"2024-03-29T14:27:25.265063Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"acc: 73.750000\n","output_type":"stream"}]},{"cell_type":"code","source":"class patchify(nn.Module):\n    def __init__(self, patch_size, embedding_size):\n        super(patchify, self).__init__()\n        \n        self.p = patch_size\n        self.lin = nn.Linear(3*(patch_size**2), embedding_size)\n        \n    def forward(self, x):\n        \n        b, c, h, w = x.size()\n        out = x.view(b, c, h//self.p, self.p, w//self.p, self.p)\n        out = out.permute(0, 2, 4, 1, 3, 5).contiguous()\n        out = out.view(b, -1, c*(self.p**2))\n        out = self.lin(out)\n        \n        return out\n\nclass single_self_attention(nn.Module):\n    \n    def __init__(self, D):\n        super(single_self_attention, self).__init__()\n        \n        self.Normalizer = nn.LayerNorm(normalized_shape = D, device = device)\n        self.query = nn.Linear(D, D//8, device = device)\n        self.key = nn.Linear(D, D//8, device = device)\n        self.value = nn.Linear(D, D//8, device = device)\n        self.residue = nn.Linear(D//8, D, device = device)\n        \n    def forward(self, x):\n        \n        out = self.Normalizer(x)          \n        q = self.query(out)\n        k = self.key(out)\n        \n        qkt = q@k.transpose(0,1)\n        qktsm = F.softmax(qkt, dim = 1)\n        v = self.value(out)\n        \n        y = qktsm@v\n        o = self.residue(y)\n        \n        return o       \n\nclass self_attention_multihead(nn.Module):\n    def __init__(self, embedding_size, heads):\n        super(self_attention_multihead, self).__init__()\n        \n        self.attention_heads = []\n        self.embedding_size = embedding_size\n        self.heads = heads\n        \n        for i in range(heads):            \n            self.attention_heads.append(single_self_attention(embedding_size//heads))\n            \n        \n    def forward(self, x):\n        \n        ret = torch.zeros(x.shape).to(device)\n        \n        for i in range(x.shape[0]):\n            \n            size = x[i].shape\n\n            multihead_ret = torch.zeros(size[0], self.heads, self.embedding_size//self.heads).to(device)\n\n            temp = x[i].view(size[0], self.heads, self.embedding_size//self.heads)        \n            for j in range(self.heads):   \n                multihead_ret[:,j,:] = self.attention_heads[j](temp[:,j,:])\n\n            ret[i] = multihead_ret.view(size) + temp.view(size)\n        \n        return ret\n    \nclass encoder_block(nn.Module):\n    def __init__(self, embedding_size, heads, MLP_hidden_size):\n        super(encoder_block, self).__init__()\n        \n        self.MHSA = self_attention_multihead(embedding_size, heads)\n        self.Normalizer = nn.LayerNorm(normalized_shape = embedding_size)\n        self.Lin1 = nn.Linear(embedding_size, MLP_hidden_size)\n        self.Lin2 = nn.Linear(MLP_hidden_size, embedding_size)\n        self.Relu = nn.ReLU()\n        \n    def forward(self, x):\n        \n        out = self.MHSA(x)\n        out = self.Normalizer(out)\n        out = self.Relu(self.Lin1(out))\n        out = self.Relu(self.Lin2(out))\n        \n        return out\n\nclass ViTLike(nn.Module):\n    def __init__(self, patch_size, embedding_size, heads):\n        super(ViTLike, self).__init__()\n        \n        self.patchit = patchify(patch_size, embedding_size)\n        \n        self.clse = nn.Parameter(data=torch.randn(1, 1, embedding_size), requires_grad=True)\n        \n        self.pos = nn.Parameter(data=torch.randn(1, 65, embedding_size),\n                                               requires_grad=True)\n        \n        self.eb1 = encoder_block(embedding_size, heads, 2000)\n        self.eb2 = encoder_block(embedding_size, heads, 2000)\n        self.eb3 = encoder_block(embedding_size, heads, 2000)\n        \n        self.l1 = nn.Linear(embedding_size, 256)\n        self.l2 = nn.Linear(256, 84)\n        self.l3 = nn.Linear(84, 10)\n        self.Relu = nn.ReLU()\n        \n        \n    def forward(self, x):\n        \n        class_token = self.clse.expand(batch_size, -1, -1)\n        \n        out = self.patchit(x)\n        \n        out = torch.cat((class_token, out), dim=1)\n        out = self.pos + out                \n        \n        out = self.eb1(out)\n        out = self.eb2(out)\n        out = self.eb3(out)\n        \n        out = out[:,0,:]\n        \n        out = self.Relu(self.l1(out))\n        out = self.Relu(self.l2(out))\n        out = self.l3(out)\n        \n        return out\n    \nModel2 = ViTLike(4, 128, 4).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:56:37.457548Z","iopub.execute_input":"2024-03-29T15:56:37.458141Z","iopub.status.idle":"2024-03-29T15:56:37.517034Z","shell.execute_reply.started":"2024-03-29T15:56:37.458111Z","shell.execute_reply":"2024-03-29T15:56:37.516066Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"Model2","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:57:38.952874Z","iopub.execute_input":"2024-03-29T15:57:38.953265Z","iopub.status.idle":"2024-03-29T15:57:38.960052Z","shell.execute_reply.started":"2024-03-29T15:57:38.953237Z","shell.execute_reply":"2024-03-29T15:57:38.959009Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"ViTLike(\n  (patchit): patchify(\n    (lin): Linear(in_features=48, out_features=128, bias=True)\n  )\n  (eb1): encoder_block(\n    (MHSA): self_attention_multihead()\n    (Normalizer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (Lin1): Linear(in_features=128, out_features=2000, bias=True)\n    (Lin2): Linear(in_features=2000, out_features=128, bias=True)\n    (Relu): ReLU()\n  )\n  (eb2): encoder_block(\n    (MHSA): self_attention_multihead()\n    (Normalizer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (Lin1): Linear(in_features=128, out_features=2000, bias=True)\n    (Lin2): Linear(in_features=2000, out_features=128, bias=True)\n    (Relu): ReLU()\n  )\n  (eb3): encoder_block(\n    (MHSA): self_attention_multihead()\n    (Normalizer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (Lin1): Linear(in_features=128, out_features=2000, bias=True)\n    (Lin2): Linear(in_features=2000, out_features=128, bias=True)\n    (Relu): ReLU()\n  )\n  (l1): Linear(in_features=128, out_features=256, bias=True)\n  (l2): Linear(in_features=256, out_features=84, bias=True)\n  (l3): Linear(in_features=84, out_features=10, bias=True)\n  (Relu): ReLU()\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(Model2.parameters(), lr =learning_rate)\nnum_epochs = 5\n\n\ntotal_steps = len(train_loader)\n\nfor epoch in tqdm(range(num_epochs)):\n    for i , (images, label) in enumerate(train_loader):\n\n        images = images.to(device)\n        label = label.to(device)\n\n        outputs = Model2(images)\n        loss = criterion(outputs, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n#         print(i)\n\n        if(i+1) % 1000 == 0:\n            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{total_steps}, loss = {loss.item():5f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-29T16:11:22.007044Z","iopub.execute_input":"2024-03-29T16:11:22.007410Z","iopub.status.idle":"2024-03-29T17:12:39.985694Z","shell.execute_reply.started":"2024-03-29T16:11:22.007383Z","shell.execute_reply":"2024-03-29T17:12:39.984577Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"  0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"epoch 1 / 5, step 1000/3125, loss = 1.852911\nepoch 1 / 5, step 2000/3125, loss = 1.674202\nepoch 1 / 5, step 3000/3125, loss = 1.575201\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 1/5 [12:14<48:59, 734.78s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 2 / 5, step 1000/3125, loss = 1.469540\nepoch 2 / 5, step 2000/3125, loss = 1.704446\nepoch 2 / 5, step 3000/3125, loss = 1.633661\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 2/5 [24:31<36:48, 736.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 3 / 5, step 1000/3125, loss = 1.363106\nepoch 3 / 5, step 2000/3125, loss = 1.922007\nepoch 3 / 5, step 3000/3125, loss = 1.220276\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 3/5 [36:47<24:32, 736.08s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 4 / 5, step 1000/3125, loss = 1.577268\nepoch 4 / 5, step 2000/3125, loss = 1.934319\nepoch 4 / 5, step 3000/3125, loss = 1.212567\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 4/5 [49:06<12:16, 736.98s/it]","output_type":"stream"},{"name":"stdout","text":"epoch 5 / 5, step 1000/3125, loss = 1.498479\nepoch 5 / 5, step 2000/3125, loss = 1.787369\nepoch 5 / 5, step 3000/3125, loss = 1.596136\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5/5 [1:01:17<00:00, 735.59s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n\n    n_c = 0\n    n_t = 0\n    \n    op = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = Model2(images)\n\n        _, predictions = torch.max(outputs.data, 1)\n        n_t += labels.shape[0]\n        n_c += (predictions == labels).sum().item()\n\n    acc = 100*n_c/n_t\n    print(f'acc: {acc:4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-29T17:14:12.501463Z","iopub.execute_input":"2024-03-29T17:14:12.502278Z","iopub.status.idle":"2024-03-29T17:17:55.446294Z","shell.execute_reply.started":"2024-03-29T17:14:12.502244Z","shell.execute_reply":"2024-03-29T17:17:55.445327Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"acc: 50.564000\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n\n    n_c = 0\n    n_t = 0\n    \n    op = 0\n\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = Model2(images)\n#         op = outputs\n#         break\n\n        _, predictions = torch.max(outputs.data, 1)\n        n_t += labels.shape[0]\n        n_c += (predictions == labels).sum().item()\n\n    acc = 100*n_c/n_t\n    print(f'acc: {acc:4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-29T17:18:41.205256Z","iopub.execute_input":"2024-03-29T17:18:41.205631Z","iopub.status.idle":"2024-03-29T17:19:25.846539Z","shell.execute_reply.started":"2024-03-29T17:18:41.205603Z","shell.execute_reply":"2024-03-29T17:19:25.845551Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"acc: 49.050000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that the accuracy of the Vision Transformer like model is lesser than that of the model involving convolutions and self attention.\n\nIt is known that encoder only models are data hungry and require a huge amount of data for them to overpower the convolutional models.\n\nAlso, only 5 epochs have been done for the vision transformer model owing for its lesser accuracy.","metadata":{}}]}